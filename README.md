# Stock-Price-Prediction-using-KNN-algo-in-ML
Recent business research interests concentrated on areas of future predictions of stock prices movements which make it challenging and demanding. Researchers, business communities, and interested users who assume that future occurrence depends on present and past data, are keen to identify the stock price prediction of movements in stock markets. . Predicting market prices are seen as problematical, and as explained in the efficient market hypotheses (EMH) that was put forward by Fama (1990), the EMH is considered as bridging the gap between financial information and the financial market; it also affirms that the fluctuations in prices are only a result of newly available information; and that all available information reflected in market prices. We applied k-nearest neighbour algorithm in order to predict stock prices for a sample of five major companies listed on the NASDAQ stock market to assist investors, management, decision makers, and users in making correct and informed investments decisions. According to the results, the k-NN algorithm is mildly robust with a good accuracy; consequently, the results were rational and also reasonable. In addition, depending on the actual stock prices data; the prediction results were close and fairly parallel to actual stock prices. We implemented the k-NN algorithm from scratch on python 2.7 to conduct the experiments for the project. k-NN is an instance-based, competitive learning, and lazy learning algorithm. Instance based algorithms, sometimes called memory-based learning, are those algorithms that, instead of performing explicit generalization, use the instances seen in the training as a comparison standard. For k-NN, the entire training dataset is the model. When a prediction is required for an unseen data instance, the k-NN algorithm will search through the training dataset for the k-most similar instances. k-NN is a competitive learning model because a majority vote is performed among the selected k records to determine the class label and then assigned it to the query record. k-NN is considered a lazy learning that does not build a model or function previously, but yields the closest k records of the training data set that have the highest similarity to the test (i.e., query record). The prediction attribute of the most similar instances is summarized and returned as the prediction for the unseen instance. The similarity measure is dependent on the type of data. For real-valued data, the Euclidean distance can be used. Other types of data such as categorical or binary data, Hamming distance can be used. In the case of regression problems, the average of the predicted attribute may be returned. In the case of classification, the most prevalent class may be returned. 
